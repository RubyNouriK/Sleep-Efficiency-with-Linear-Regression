---
title: "Sleep Efficiency - DATA 603 Project"
author:
- Ali Afkhami (30271805)
- Daniela Ma√±ozca Cruz (30262558)
- Evan Losier (30022571)
- Luisa Alejandra Sierra Guerra (30261956)
- Ruby Nouri Kermani (30261323)
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
    toc: yes
  toc: yes
geometry: margin=1in 
fontsize: 11pt
---


## Libraries and Packages

```{r}
suppressWarnings({
  library(lubridate)
  library(mctest)
  library(olsrr)
  library(ggplot2)
  library(lmtest)
})
```



## Uploading the data set

Read the data set:

```{r}
sleep_data = read.csv("Sleep_Efficiency.csv")
head(sleep_data)
```

Dataset info:

```{r}
str(sleep_data)
```

## Data Cleaning

Removing extra spaces

```{r}
sleep_data$Bedtime = trimws(sleep_data$Bedtime)
sleep_data$Wakeup.time = trimws(sleep_data$Wakeup.time)
```


Filling any "NA" values with "0"

```{r}
sleep_data[is.na(sleep_data)] = 0
```


## Data Transformation

Converting values from 'Bedtime' and 'Wakeup.time' columns from "chr" format to datetime.

Meaning, we would like to see the values in the format of YEAR-MONTH-DAY and military time.

```{r}
sleep_data$Bedtime = mdy_hms(sleep_data$Bedtime, tz="UTC")
sleep_data$Wakeup.time = mdy_hms(sleep_data$Wakeup.time, tz="UTC")
```

Now, we check the values format for 'Bedtime' and 'Wakeup.time' columns

```{r}
str(sleep_data)
```

```{r}
head(sleep_data)
```

Next, we will extract 'hour:minute' values from the 'Bedtime' and 'Wakeup.time' columns and save them on new columns name 'Bedtime_hour' and 'Wakeup.time_hour'.

```{r}
sleep_data$Bedtime_hour = format(sleep_data$Bedtime, format = "%H:%M")
sleep_data$Wakeup.time_hour = format(sleep_data$Wakeup.time, format = "%H:%M")
```

```{r}
head(sleep_data)
```

We believe that analyzing Bedtime and Wake-up time will be important, but since our data was in a format of time of day where 1:00AM would be treated as a lower number than 11:00PM, we wanted to change it so that we're able to work on the data and make a linear model out of it. 

Therefore, we will make new columns named 'Bedtime_shifted' and 'Wakeuptime_shifted' to calculate the time difference between the person that went to bed the earliest and the time that other participants went to bed.

The reference point is 1 hour before the time that the earliest person went to bed. For example, if the earliest person went to bed at 6 pm, their time shifted will be **1**, and the second person that went to bed is at 9 pm will be **4**.

```{r}
# Extract hour and minute from bedtime
sleep_data$Bedtime_hour = hour(sleep_data$Bedtime) + (minute(sleep_data$Bedtime) / 60)

# Extract hour and minute from wakeup time
sleep_data$Wakeup.time_hour = hour(sleep_data$Wakeup.time) + (minute(sleep_data$Wakeup.time) / 60)
```


```{r}
# Ensure Bedtime_hour and wakeup.time_hour are numeric
sleep_data$Bedtime_hour = as.numeric(sleep_data$Bedtime_hour)
sleep_data$Wakeup.time_hour = as.numeric(sleep_data$Wakeup.time_hour)

# Adjust bedtimes: Add 24 hours if bedtime is after midnight (before noon)
sleep_data$Bedtime_adj = ifelse(sleep_data$Bedtime_hour < 12, 
                                 sleep_data$Bedtime_hour + 24, 
                                 sleep_data$Bedtime_hour)

# Doing the same for the wakeup.time_hour
sleep_data$wakeup.time_adj = ifelse(sleep_data$Wakeup.time_hour < 12, 
                                 sleep_data$Wakeup.time_hour + 24, 
                                 sleep_data$Wakeup.time_hour)


# Find the new earliest bedtime and set the reference
earliest_bedtime = min(sleep_data$Bedtime_adj, na.rm = TRUE)
# 1 hour before the earliest bedtime
reference_time = earliest_bedtime - 1

# Compute shifted bedtime
# (reference time of the wake up time will be the same as the bedtime)
sleep_data$Bedtime_shifted = sleep_data$Bedtime_adj - reference_time
sleep_data$Wakeuptime_shifted = sleep_data$wakeup.time_adj - reference_time
```


```{r}
# Drop the columns "Bedtime_hour","Wakeup.time_hour", "Bedtime_adj" and "wakeup.time_adj"

drop = c("Bedtime_hour","Wakeup.time_hour","Bedtime_adj","wakeup.time_adj")

sleep_data = sleep_data[,!(names(sleep_data) %in% drop)]
```


```{r}
head(sleep_data)
```


**Before we start with the analysis, we set the alpha to 0.05**


## Full Additive Model


```{r}
full_model = lm(Sleep.efficiency ~ Age + factor(Gender) + Sleep.duration + REM.sleep.percentage + Deep.sleep.percentage + Light.sleep.percentage + Awakenings + Caffeine.consumption + Alcohol.consumption + factor(Smoking.status) + Exercise.frequency + Bedtime_shifted + Wakeuptime_shifted, data = sleep_data)

summary(full_model)
```

## Checking Assumption No. 1 - Linearity

```{r}
ggplot(full_model, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)
```

Based on the plot, the full additive model appears nonlinear. This suggest the linearity assumption may be violated, and adding interactions or higher order terms might be better capture the underlying trend.


## Checking Assumption No. 2 - Multicollinearity


```{r}
pairs(~Sleep.efficiency + REM.sleep.percentage + Light.sleep.percentage + Deep.sleep.percentage, data = sleep_data)
```

```{r}
suppressWarnings({
  imcdiag(full_model, method="VIF")
})
```

We checked their variance inflation factor (VIF) and saw a correlation between *'REM.sleep.percentage'*, *'Deep.sleep.percentage'*, and *'Light.sleep.percentage'*.

Therefore, we tried to check all possible models that didn't have a correlation between the predictors. 

We checked a model that had *'Deep.sleep.percentage' & 'REM.sleep.percentage'*, a model with *'REM.sleep.percentage' & 'Light.sleep.percentage'*, and a model with *'Deep.sleep.percentage' & 'Light.sleep.percentage'*. 

```{r}
full_model_rem_deep = lm(Sleep.efficiency ~ Age + factor(Gender) + Sleep.duration + REM.sleep.percentage+ Deep.sleep.percentage + Awakenings + Caffeine.consumption + Alcohol.consumption + factor(Smoking.status) + Exercise.frequency + Bedtime_shifted + Wakeuptime_shifted, data = sleep_data)

imcdiag(full_model_rem_deep, method="VIF")
summary(full_model_rem_deep)
```


All models have the same adjusted R-squared and RSE.

Based on our results, the model with 'REM.sleep.percentage' and 'Deep.sleep.percentage' had no multicollinearity between the predictors and both predictors were significant. 

Therefore, we choose the model with **'REM.sleep.percentage' and 'Deep.sleep.percentage'**. 


## Model Selection with Step-wise Procedure

Using the *Step Wise Regression Procedure* to create the best-fit additive model, we used a p-value for inclusion in the model of **0.05** and a p-value for removal from the model of **0.3**. In addition, we conducted an individual coefficients t-test with the remaining variables identified by the stepwise regression procedure, using the following hypotheses:  


$H_{0}: \beta_{i} = 0$

$H_{a}: \beta_{i} \neq 0$

$i$ = Gender, Sleep.duration, Caffeine.consumption, Bedtime_shifted, Wakeuptime_shifted

```{r}
full_model_rem_deep = lm(Sleep.efficiency ~ Age + Gender + Sleep.duration + REM.sleep.percentage+ Deep.sleep.percentage + Awakenings + Caffeine.consumption + Alcohol.consumption + Smoking.status + Exercise.frequency + Bedtime_shifted + Wakeuptime_shifted, data = sleep_data)

full_model_rem_deep_wise = ols_step_both_p(full_model_rem_deep,p_enter = 0.05, p_remove = 0.3, details=FALSE)

summary(full_model_rem_deep_wise$model)
```

With this step-wise procedure, the additive regression model is:

$$
\begin{aligned}
\widehat{Sleep.efficiency} &= 0.3526447 + 0.0057675 {X_{Deep.sleep.percentage}}  + 0.0072077 {X_{REM.sleep.percentage}} \\
&\quad - 0.0413003 {X_{Smoking.status(YES)}}  + 0.0007721 {X_{Age}} - 0.0319148 {X_{Awakenings}} \\
&\quad - 0.0059629 {X_{Alcohol.consumption}} + 0.0047631 {X_{Exercise.frequency}}
\end{aligned}
$$



Also, we checked it with the All Possible-Regression selection procedure.

*(The following code will take minimum of 2 minutes to run, once you run it you don't need to run it again)*

```{r}
full_model_rem_deep = lm(formula = Sleep.efficiency ~ Age + (Gender) + Sleep.duration + 
    REM.sleep.percentage + Deep.sleep.percentage + Awakenings + 
    Caffeine.consumption + Alcohol.consumption + (Smoking.status) + 
    Exercise.frequency + Bedtime_shifted + Wakeuptime_shifted, 
    data = sleep_data)
ExecSubsets=ols_step_best_subset(full_model_rem_deep, details=TRUE)
```

```{r}
ExecSubsets
```


Based on the output, the best models based on high adjusted R-squared are the models with 7 or 8 predictors and both have a $R^{2}_{adj} = 0.7976$.

The best model based on lowest AIC is the model with a $AIC = -1238.0046$, which has 7 predictors.

Therefore, we will use the model with **7** predictors:

* Age 
* REM.sleep.percentage 
* Deep.sleep.percentage 
* Awakenings 
* Alcohol.consumption 
* Smoking.status 
* Exercise.frequency

```{r}
fulladditivemodel = lm(Sleep.efficiency ~ Age + REM.sleep.percentage + Deep.sleep.percentage + Awakenings + Alcohol.consumption + factor(Smoking.status) + Exercise.frequency, data=sleep_data)

summary(fulladditivemodel)
```


Then, we performed a global F-test to decide if the reduced model is a better fit for the data than the null model.

$H_{0}:$ the reduced model is preferred

$H_{a}:$ the full model is preferred

$i$ = Gender, Sleep.duration, Caffeine.consumption, Bedtime_shifted, Wakeuptime_shifted


```{r}
anova(fulladditivemodel, full_model)
```

Since the $p_{value} = 0.8113$ is higher than $\alpha$, we fail to reject the null hypothesis and we would prefer the reduced model.

Our final full additive model is:

$$
\begin{aligned}
\widehat{Sleep.efficiency} &= 0.3526447 + 0.0057675 {X_{Deep.sleep.percentage}}  + 0.0072077 {X_{REM.sleep.percentage}} \\
&\quad - 0.0413003 {X_{Smoking.status(YES)}}  + 0.0007721 {X_{Age}} - 0.0319148 {X_{Awakenings}} \\
&\quad - 0.0059629 {X_{Alcohol.consumption}} + 0.0047631 {X_{Exercise.frequency}}
\end{aligned}
$$

## Interaction Models

First, we test the full interaction model.

```{r}
full_interaction_model = lm(Sleep.efficiency ~ (Age + REM.sleep.percentage + Deep.sleep.percentage + Awakenings + Alcohol.consumption + Smoking.status + Exercise.frequency)^2, data=sleep_data)

summary(full_interaction_model)
```



$H_{0}: \beta_{i} = 0$

$H_{a}: \beta_{i} \neq 0$

where $i$ is any pairwise combination of variables.


Next, using the *Step Wise Regression Procedure* to create the best-fit interaction model, we used a p-value for inclusion in the model of **0.05** and a p-value for removal from the model of **0.1**. 


```{r}
interaction_step = ols_step_both_p(full_interaction_model, p_enter=0.05, p_remove=0.1, details=FALSE)
summary(interaction_step$model)
```

The results of the step-wise procedure result in a model with the following **4** interactions:

* Smoking.status * Deep.sleep.percentage
* Awakenings * Deep.sleep.percentage
* Age * Deep.sleep.percentage
* REM.sleep.percentage * Awakenings

With this final interaction model, we will re-test the linearity assumption.

```{r}
interactionmodel = lm(Sleep.efficiency ~ REM.sleep.percentage + Age + Awakenings + Exercise.frequency + factor(Smoking.status) + Alcohol.consumption + Deep.sleep.percentage + factor(Smoking.status)*Deep.sleep.percentage + Awakenings*Deep.sleep.percentage + Age*Deep.sleep.percentage + REM.sleep.percentage*Awakenings, data = sleep_data)

ggplot(interactionmodel, aes(x=.fitted, y=.resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept=0)
```

There doesn't seem to be any obvious pattern in the residuals plot, but the data does seem to have a curve for both low and high fitted values. We will check a pairs plot to see if any predictors might have higher-order patterns.


```{r}
pairs(~Sleep.efficiency + Age + REM.sleep.percentage + Deep.sleep.percentage, data=sleep_data)

pairs(~Sleep.efficiency + Awakenings + Alcohol.consumption + factor(Smoking.status) + Exercise.frequency, data=sleep_data)
```



## Higher-order Models

$H_{0}:$ Heteroscedasticity is not present.

$H_{a}:$ Heteroscedasticity is present.


Since we don't want to over-fit our model or invalidate any assumptions, we decided to investigate three different promising higher order models going forward to see which one ends up meeting the most criteria.

### Model 1:

```{r}
model1 = lm(Sleep.efficiency ~ REM.sleep.percentage + Age + Awakenings + Exercise.frequency + Smoking.status + Alcohol.consumption + Deep.sleep.percentage + Smoking.status*Deep.sleep.percentage + Awakenings*Deep.sleep.percentage + Age*Deep.sleep.percentage + REM.sleep.percentage*Awakenings + I(Deep.sleep.percentage^2) + I(Deep.sleep.percentage^3) + I(Awakenings^2) + I(Awakenings^3) + I(Awakenings^4), data=sleep_data)

summary(model1)
bptest(model1)
shapiro.test(residuals(model1))
```

In model 1, by using the Breusch-Pagan test, and getting a $p_{value} = 0.03796$ which is lower than $\alpha$, we reject the null hypothesis and conclude that our model is heteroscedastic and there is a problem with the homoscedasticity assumption.

### Model 2:

```{r}
model2 = lm(Sleep.efficiency ~ REM.sleep.percentage + Age + Awakenings + Exercise.frequency + Smoking.status + Alcohol.consumption + Deep.sleep.percentage + Smoking.status*Deep.sleep.percentage + Awakenings*Deep.sleep.percentage + Age*Deep.sleep.percentage + REM.sleep.percentage*Awakenings + I(Awakenings^2) + I(Awakenings^3) + I(Awakenings^4) + I(Deep.sleep.percentage^2) + I(Deep.sleep.percentage^3) + I(Age^2), data = sleep_data)

summary(model2)
bptest(model2)
shapiro.test(residuals(model2))
```

In model 2, by using the Breusch-Pagan test, and getting a $p_{value} = 0.0523$ which is higher than $\alpha$, we fail to reject the null hypothesis and conclude that our model is not heteroscedastic and there is not a problem with the homoscedasticity assumption.

### Model 3:

```{r}
model3 = lm(Sleep.efficiency~REM.sleep.percentage+Age+Awakenings+Exercise.frequency+Smoking.status+Alcohol.consumption+Deep.sleep.percentage+Smoking.status*Deep.sleep.percentage+Awakenings*Deep.sleep.percentage+Age*Deep.sleep.percentage+REM.sleep.percentage*Awakenings+I(Age^2)+I(Age^3)+I(Age^4)+I(Age^5)+I(Deep.sleep.percentage^2)+I(Deep.sleep.percentage^3)+I(Deep.sleep.percentage^4)+I(Deep.sleep.percentage^5), data=sleep_data)
summary(model3)
bptest(model3)
shapiro.test(residuals(model3))
```

In model 3, by using the Breusch-Pagan test, and getting a $p_{value} = 0.2247$ which is higher than $\alpha$, we fail to reject the null hypothesis and conclude that our model is not heteroscedastic and there is not a problem with the homoscedasticity assumption.

All of these models have all significant predictors and a high adjusted R-squared value. Model 2 has one predictor that isn't quite significant, but it is being kept in the model to compare for all future assumptions.

Once again we must re-test the linearity assumption, but this time with the full interaction model with higher-order terms.

```{r}
ggplot(model1, aes(x=.fitted, y=.resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept=0)

ggplot(model2, aes(x=.fitted, y=.resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept=0)

ggplot(model3, aes(x=.fitted, y=.resid)) +
geom_point() +
geom_smooth() +
geom_hline(yintercept=0)
```

Based on these final graphs, we can see that adding higher-order terms has improved the linearity of all the models and we can proceed with checking other assumptions.


## Checking Assumption No. 3 - Outliers

```{r}
plot(model1 ,which=5)
plot(model2, which=5)
plot(model3, which=5)
```

None of the data points for any of the models are considered outliers because they all have small Cook's distance values. This means that there are no points with abnormally high influence on the outcome of the model and we don't have to remove any outliers.


## Checking Assumption No. 4 - Independence 

Since each row in the data is associated with a unique test subject and are not related to each other in a time-series, we can safely assume that the measurements are independent. If we suspected the measurements might not be independent, we could plot error terms in the order in which they occur in the dataset and try to observe any pattern in the plot.


## Checking Assumption No. 5 - Equal Variance 

$H_{0}:$ There's common variance

$H_{a}:$ There's not common variance

```{r}
ggplot(model1, aes(x=.fitted, y=.resid)) +
geom_point(colour = "blue") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "maroon") +
ggtitle("Residual Plot: Residual vs Fitted Values (model1)")

ggplot(model2, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4") +
ggtitle("Residual Plot: Residual vs Fitted Values (model2)")

ggplot(model3, aes(x=.fitted, y=.resid)) +
geom_point(colour = "cyan3") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "orange2") +
ggtitle("Residual Plot: Residual vs Fitted Values (model3)")

ggplot(model1, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
  geom_point(colour = "blue") +
  geom_hline(yintercept = 0) +
  geom_smooth( colour = "maroon")+
   ggtitle("Scale-Loc. plot : Standardized Residual vs Fitted Values (model1)")

ggplot(model2, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
  geom_point(colour = "purple") +
  geom_hline(yintercept = 0) +
  geom_smooth( colour = "green4")+
   ggtitle("Scale-Loc. plot : Standardized Residual vs Fitted Values (model2)")

ggplot(model3, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
  geom_point(colour = "cyan3") +
  geom_hline(yintercept = 0) +
  geom_smooth( colour = "orange2")+
   ggtitle("Scale-Loc. plot : Standardized Residual vs Fitted Values (model3)")
```

These residual and scale-location plots seem to have slight patterns, suggesting the models might have heteroscedasticity. We need to investigate further by using the Breusch-Pagan test with a null hypothesis that the models have homoscedasticity and the alternate hypothesis being that the models have heteroscedasticity.

```{r}
bptest(model1)
bptest(model2)
bptest(model3)
```

The results from the bp-tests show that model 1 has heteroscedasticity while model 2 and model 3 have homoscedasticity, with **model 3** being the best option here.


## Checking Assumption No. 6 - Normality

$H_{0}:$ the sample data are significantly normally distributed

$H_{a}:$ the sample data are not significantly normally distributed

```{r}
ggplot(sleep_data, aes(sample=model1$residuals)) +
stat_qq() +
stat_qq_line()+labs(title="Model 1")

ggplot(sleep_data, aes(sample=model2$residuals)) +
stat_qq() +
stat_qq_line()+labs(title="Model 2")

ggplot(sleep_data, aes(sample=model3$residuals)) +
stat_qq() +
stat_qq_line()+labs(title="Model 3")
```

```{r}
shapiro.test(residuals(model1))
shapiro.test(residuals(model2))
shapiro.test(residuals(model3))
```


According to the stat-QQ line plot, there is a noticeable bow shaped pattern and kurtosis of the diagonal points, suggesting that the residuals are not normally distributed. We can also confirm this by running a Shapiro-Wilk test for normality with the null hypothesis being that the residuals are normally distributed and the the alternate hypothesis being that the residuals are not normally distributed. The p-values for all models are very low, so we conclude that the residuals are not normally distributed.


## Final Model

Therefore the final model will be:

$$
\begin{aligned}
\widehat{Sleep.efficiency} &= -2.308 + 0.0058 X_{REM.sleep.percentage} + 0.1067 X_{Age} + 0.0779 X_{Awakenings} + 0.0071 X_{Exercise.frequency} \\
& - 0.1514 X_{Smoking.statusYes} - 0.0062 X_{Alcohol.consumption} + 0.2858 X_{Deep.sleep.percentage} \\
& - 0.0061 X_{Age}^2 + 0.00017 X_{Age}^3 - 0.00000216 X_{Age}^4 + 0.0000000105 X_{Age}^5 \\
& - 0.0148 X_{Deep.sleep.percentage}^2 + 0.00036 X_{Deep.sleep.percentage}^3 - 0.00000412 X_{Deep.sleep.percentage}^4 + 0.0000000177 X_{Deep.sleep.percentage}^5 \\
& + 0.0022 X_{Smoking.statusYes} * X_{Deep.sleep.percentage} \\
& - 0.00099 X_{Awakenings} * X_{Deep.sleep.percentage} \\
& - 0.0000261 X_{Age} * X_{Deep.sleep.percentage} \\
& - 0.0024 X_{REM.sleep.percentage} * X_{Awakenings}
\end{aligned}
$$

Also, the sub model for smokers:

$$
\begin{aligned}
\widehat{Sleep.efficiency} &= -2.4594 + 0.0058 X_{REM.sleep.percentage} + 0.1067 X_{Age} + 0.0779 X_{Awakenings} + 0.0071 X_{Exercise.frequency} \\
& - 0.0062 X_{Alcohol.consumption} + 0.2880 X_{Deep.sleep.percentage} \\
& - 0.0061 X_{Age}^2 + 0.00017 X_{Age}^3 - 0.00000216 X_{Age}^4 + 0.0000000105 X_{Age}^5 \\
& - 0.0148 X_{Deep.sleep.percentage}^2 + 0.00036 X_{Deep.sleep.percentage}^3 - 0.00000412 X_{Deep.sleep.percentage}^4 + 0.0000000177 X_{Deep.sleep.percentage}^5 \\
& - 0.00099 X_{Awakenings} * X_{Deep.sleep.percentage} \\
& - 0.0000261 X_{Age} * X_{Deep.sleep.percentage} \\
& - 0.0024 X_{REM.sleep.percentage} * X_{Awakenings}
\end{aligned}
$$


and the sub model for non-smokers:

$$
\begin{aligned}
\widehat{Sleep.efficiency} &= -2.308 + 0.0058 X_{REM.sleep.percentage} + 0.1067 X_{Age} + 0.0779 X_{Awakenings} + 0.0071 X_{Exercise.frequency} \\
& - 0.0062 X_{Alcohol.consumption} + 0.2858 X_{Deep.sleep.percentage} \\
& - 0.0061 X_{Age}^2 + 0.00017 X_{Age}^3 - 0.00000216 X_{Age}^4 + 0.0000000105 X_{Age}^5 \\
& - 0.0148 X_{Deep.sleep.percentage}^2 + 0.00036 X_{Deep.sleep.percentage}^3 - 0.00000412 X_{Deep.sleep.percentage}^4 + 0.0000000177 X_{Deep.sleep.percentage}^5 \\
& - 0.00099 X_{Awakenings} * X_{Deep.sleep.percentage} \\
& - 0.0000261 X_{Age} * X_{Deep.sleep.percentage} \\
& - 0.0024 X_{REM.sleep.percentage} * X_{Awakenings}
\end{aligned}
$$
